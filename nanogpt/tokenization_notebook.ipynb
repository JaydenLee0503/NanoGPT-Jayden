{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c24ced5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "hi hi hi hi hi my name is Jayden Lee and I like deep learning and math. I want to build AI.\n",
      "length: 91\n",
      "---\n",
      "[104, 105, 32, 104, 105, 32, 104, 105, 32, 104, 105, 32, 104, 105, 32, 109, 121, 32, 110, 97, 109, 101, 32, 105, 115, 32, 74, 97, 121, 100, 101, 110, 32, 76, 101, 101, 32, 97, 110, 100, 32, 73, 32, 108, 105, 107, 101, 32, 100, 101, 101, 112, 32, 108, 101, 97, 114, 110, 105, 110, 103, 32, 97, 110, 100, 32, 109, 97, 116, 104, 46, 32, 73, 32, 119, 97, 110, 116, 32, 116, 111, 32, 98, 117, 105, 108, 100, 32, 65, 73, 46]\n",
      "length: 91\n"
     ]
    }
   ],
   "source": [
    "text = \"hi hi hi hi hi my name is Jayden Lee and I like deep learning and math. I want to build AI.\"\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))\n",
    "print('---')\n",
    "print(text)\n",
    "print(\"length:\", len(text))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print(\"length:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb454e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, (105, 32)), (5, (104, 105)), (4, (32, 104)), (3, (101, 32)), (3, (100, 32)), (3, (97, 110)), (2, (110, 100)), (2, (101, 101)), (2, (100, 101)), (2, (73, 32)), (2, (32, 109)), (2, (32, 108)), (2, (32, 97)), (2, (32, 73)), (1, (121, 100)), (1, (121, 32)), (1, (119, 97)), (1, (117, 105)), (1, (116, 111)), (1, (116, 104)), (1, (116, 32)), (1, (115, 32)), (1, (114, 110)), (1, (112, 32)), (1, (111, 32)), (1, (110, 116)), (1, (110, 105)), (1, (110, 103)), (1, (110, 97)), (1, (110, 32)), (1, (109, 121)), (1, (109, 101)), (1, (109, 97)), (1, (108, 105)), (1, (108, 101)), (1, (108, 100)), (1, (107, 101)), (1, (105, 115)), (1, (105, 110)), (1, (105, 108)), (1, (105, 107)), (1, (104, 46)), (1, (103, 32)), (1, (101, 112)), (1, (101, 110)), (1, (101, 97)), (1, (98, 117)), (1, (97, 121)), (1, (97, 116)), (1, (97, 114)), (1, (97, 109)), (1, (76, 101)), (1, (74, 97)), (1, (73, 46)), (1, (65, 73)), (1, (46, 32)), (1, (32, 119)), (1, (32, 116)), (1, (32, 110)), (1, (32, 105)), (1, (32, 100)), (1, (32, 98)), (1, (32, 76)), (1, (32, 74)), (1, (32, 65))]\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "stats = get_stats(tokens)\n",
    "print(sorted(((v,k) for k,v in stats.items()), reverse=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82649fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 105)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9905f16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256, 32, 256, 32, 256, 32, 256, 32, 256, 32, 109, 121, 32, 110, 97, 109, 101, 32, 105, 115, 32, 74, 97, 121, 100, 101, 110, 32, 76, 101, 101, 32, 97, 110, 100, 32, 73, 32, 108, 105, 107, 101, 32, 100, 101, 101, 112, 32, 108, 101, 97, 114, 110, 105, 110, 103, 32, 97, 110, 100, 32, 109, 97, 116, 104, 46, 32, 73, 32, 119, 97, 110, 116, 32, 116, 111, 32, 98, 117, 105, 108, 100, 32, 65, 73, 46]\n",
      "[5, 6, 99, 9, 1]\n",
      "length: 86\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)  # new token id\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "tokens2 = merge(tokens, top_pair, 256)  # example with new token id 256\n",
    "print(tokens2)\n",
    "print(merge([5,6,6,7,9,1], (6,7), 99))  # example with new token id 256\n",
    "print(\"length:\", len(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b82bde2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"A programmer's work is never done. They must constantly debug, optimize, and innovate to keep up with the ever-evolving landscape of technology. But amidst the lines of code and endless coffee cups, they find joy in creating something out of nothing, solving complex problems, and bringing ideas to life. In the end, it's not just about writing code; it's about crafting experiences that can change the world. In this journey, perseverance and passion are their greatest allies. Therefore, they code on, fueled by curiosity and the thrill of discovery.\"\"\"\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf91c740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (32, 116) into a new token 256\n",
      "merging (32, 116) into a new token 257\n",
      "merging (32, 116) into a new token 258\n",
      "merging (32, 116) into a new token 259\n",
      "merging (32, 116) into a new token 260\n",
      "merging (32, 116) into a new token 261\n",
      "merging (32, 116) into a new token 262\n",
      "merging (32, 116) into a new token 263\n",
      "merging (32, 116) into a new token 264\n",
      "merging (32, 116) into a new token 265\n",
      "merging (32, 116) into a new token 266\n",
      "merging (32, 116) into a new token 267\n",
      "merging (32, 116) into a new token 268\n",
      "merging (32, 116) into a new token 269\n",
      "merging (32, 116) into a new token 270\n",
      "merging (32, 116) into a new token 271\n",
      "merging (32, 116) into a new token 272\n",
      "merging (32, 116) into a new token 273\n",
      "merging (32, 116) into a new token 274\n",
      "merging (32, 116) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)  # new token id\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "5\n",
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"merging {pair} into a new token {idx}\")\n",
    "    ids = merge(ids, top_pair, idx)\n",
    "    merges[pair] = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d25c0f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 552\n",
      "final ids length: 549\n",
      "compression ratio: 1.01\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"final ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens)/len(ids):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ddd064",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "\n",
    "def decode(ids):\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors='replace')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92d54dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(32, 116): 275}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ecdff31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 33]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float('inf')))\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "print(encode(\"hello world!\"))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0544c4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88ed2c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world', ' how', ' are', ' you', '?!!!!']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "print(re.findall(gpt2pat, \"Hello world how are you?!!!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c71bd4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'for', ' i', ' in', ' range', '(', '1', ',', ' 101', '):', '\\n   ', ' if', ' i', ' %', ' 3', ' ==', ' 0', ' and', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'FizzBuzz', '\")', '\\n   ', ' elif', ' i', ' %', ' 3', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Fizz', '\")', '\\n   ', ' elif', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Buzz', '\")', '\\n   ', ' else', ':', '\\n       ', ' print', '(', 'i', ')', '\\n']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "\"\"\"\n",
    "print(re.findall(gpt2pat, example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2912afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 220, 220, 220, 220, 220, 18435, 995, 10185]\n",
      "[260, 22691, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"         Hello world!!!\"))\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"         Hello world!!!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b00a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[260, 22691, 1917, 12340]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tiktoken \n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"         Hello world!!!\"))\n",
    "print(enc.decode(enc.encode(\"         Hello world!!!\")) == \"         Hello world!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b05549b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "207468be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"example.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\"\"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. It is language independent and treats the input text as a raw byte sequence, so it can be used for any language including English, Chinese, Japanese, Korean, etc.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e87d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "options = dict(\n",
    "    input='example.txt',\n",
    "    input_format='text',\n",
    "    model_prefix='tok400',\n",
    "    vocab_size=400,\n",
    "    model_type='bpe',\n",
    "    normalization_rule_name='identity',\n",
    "    remove_extra_whitespaces=False,\n",
    "    input_sentence_size=200000000,\n",
    "    max_sentence_length=4192,\n",
    "    seed_sentencepiece_size=1000000,\n",
    "    shuffle_input_sentence=True,\n",
    "    character_coverage=0.99995,\n",
    "    byte_fallback=True,\n",
    "    split_digits=True,\n",
    "    split_by_unicode_script=True,\n",
    "    split_by_whitespace=True,\n",
    "    split_by_number = True, \n",
    "    max_sentencepiece_length=16,\n",
    "    add_dummy_prefix=True,\n",
    "    allow_whitespace_only_pieces=True,\n",
    "\n",
    "    unk_id = 0,\n",
    "    bos_id = 1,\n",
    "    eos_id = 2,\n",
    "    pad_id = -1,\n",
    "\n",
    "    num_threads = os.cpu_count(),\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.Train(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6174c2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', 0],\n",
       " ['<s>', 1],\n",
       " ['</s>', 2],\n",
       " ['<0x00>', 3],\n",
       " ['<0x01>', 4],\n",
       " ['<0x02>', 5],\n",
       " ['<0x03>', 6],\n",
       " ['<0x04>', 7],\n",
       " ['<0x05>', 8],\n",
       " ['<0x06>', 9],\n",
       " ['<0x07>', 10],\n",
       " ['<0x08>', 11],\n",
       " ['<0x09>', 12],\n",
       " ['<0x0A>', 13],\n",
       " ['<0x0B>', 14],\n",
       " ['<0x0C>', 15],\n",
       " ['<0x0D>', 16],\n",
       " ['<0x0E>', 17],\n",
       " ['<0x0F>', 18],\n",
       " ['<0x10>', 19],\n",
       " ['<0x11>', 20],\n",
       " ['<0x12>', 21],\n",
       " ['<0x13>', 22],\n",
       " ['<0x14>', 23],\n",
       " ['<0x15>', 24],\n",
       " ['<0x16>', 25],\n",
       " ['<0x17>', 26],\n",
       " ['<0x18>', 27],\n",
       " ['<0x19>', 28],\n",
       " ['<0x1A>', 29],\n",
       " ['<0x1B>', 30],\n",
       " ['<0x1C>', 31],\n",
       " ['<0x1D>', 32],\n",
       " ['<0x1E>', 33],\n",
       " ['<0x1F>', 34],\n",
       " ['<0x20>', 35],\n",
       " ['<0x21>', 36],\n",
       " ['<0x22>', 37],\n",
       " ['<0x23>', 38],\n",
       " ['<0x24>', 39],\n",
       " ['<0x25>', 40],\n",
       " ['<0x26>', 41],\n",
       " ['<0x27>', 42],\n",
       " ['<0x28>', 43],\n",
       " ['<0x29>', 44],\n",
       " ['<0x2A>', 45],\n",
       " ['<0x2B>', 46],\n",
       " ['<0x2C>', 47],\n",
       " ['<0x2D>', 48],\n",
       " ['<0x2E>', 49],\n",
       " ['<0x2F>', 50],\n",
       " ['<0x30>', 51],\n",
       " ['<0x31>', 52],\n",
       " ['<0x32>', 53],\n",
       " ['<0x33>', 54],\n",
       " ['<0x34>', 55],\n",
       " ['<0x35>', 56],\n",
       " ['<0x36>', 57],\n",
       " ['<0x37>', 58],\n",
       " ['<0x38>', 59],\n",
       " ['<0x39>', 60],\n",
       " ['<0x3A>', 61],\n",
       " ['<0x3B>', 62],\n",
       " ['<0x3C>', 63],\n",
       " ['<0x3D>', 64],\n",
       " ['<0x3E>', 65],\n",
       " ['<0x3F>', 66],\n",
       " ['<0x40>', 67],\n",
       " ['<0x41>', 68],\n",
       " ['<0x42>', 69],\n",
       " ['<0x43>', 70],\n",
       " ['<0x44>', 71],\n",
       " ['<0x45>', 72],\n",
       " ['<0x46>', 73],\n",
       " ['<0x47>', 74],\n",
       " ['<0x48>', 75],\n",
       " ['<0x49>', 76],\n",
       " ['<0x4A>', 77],\n",
       " ['<0x4B>', 78],\n",
       " ['<0x4C>', 79],\n",
       " ['<0x4D>', 80],\n",
       " ['<0x4E>', 81],\n",
       " ['<0x4F>', 82],\n",
       " ['<0x50>', 83],\n",
       " ['<0x51>', 84],\n",
       " ['<0x52>', 85],\n",
       " ['<0x53>', 86],\n",
       " ['<0x54>', 87],\n",
       " ['<0x55>', 88],\n",
       " ['<0x56>', 89],\n",
       " ['<0x57>', 90],\n",
       " ['<0x58>', 91],\n",
       " ['<0x59>', 92],\n",
       " ['<0x5A>', 93],\n",
       " ['<0x5B>', 94],\n",
       " ['<0x5C>', 95],\n",
       " ['<0x5D>', 96],\n",
       " ['<0x5E>', 97],\n",
       " ['<0x5F>', 98],\n",
       " ['<0x60>', 99],\n",
       " ['<0x61>', 100],\n",
       " ['<0x62>', 101],\n",
       " ['<0x63>', 102],\n",
       " ['<0x64>', 103],\n",
       " ['<0x65>', 104],\n",
       " ['<0x66>', 105],\n",
       " ['<0x67>', 106],\n",
       " ['<0x68>', 107],\n",
       " ['<0x69>', 108],\n",
       " ['<0x6A>', 109],\n",
       " ['<0x6B>', 110],\n",
       " ['<0x6C>', 111],\n",
       " ['<0x6D>', 112],\n",
       " ['<0x6E>', 113],\n",
       " ['<0x6F>', 114],\n",
       " ['<0x70>', 115],\n",
       " ['<0x71>', 116],\n",
       " ['<0x72>', 117],\n",
       " ['<0x73>', 118],\n",
       " ['<0x74>', 119],\n",
       " ['<0x75>', 120],\n",
       " ['<0x76>', 121],\n",
       " ['<0x77>', 122],\n",
       " ['<0x78>', 123],\n",
       " ['<0x79>', 124],\n",
       " ['<0x7A>', 125],\n",
       " ['<0x7B>', 126],\n",
       " ['<0x7C>', 127],\n",
       " ['<0x7D>', 128],\n",
       " ['<0x7E>', 129],\n",
       " ['<0x7F>', 130],\n",
       " ['<0x80>', 131],\n",
       " ['<0x81>', 132],\n",
       " ['<0x82>', 133],\n",
       " ['<0x83>', 134],\n",
       " ['<0x84>', 135],\n",
       " ['<0x85>', 136],\n",
       " ['<0x86>', 137],\n",
       " ['<0x87>', 138],\n",
       " ['<0x88>', 139],\n",
       " ['<0x89>', 140],\n",
       " ['<0x8A>', 141],\n",
       " ['<0x8B>', 142],\n",
       " ['<0x8C>', 143],\n",
       " ['<0x8D>', 144],\n",
       " ['<0x8E>', 145],\n",
       " ['<0x8F>', 146],\n",
       " ['<0x90>', 147],\n",
       " ['<0x91>', 148],\n",
       " ['<0x92>', 149],\n",
       " ['<0x93>', 150],\n",
       " ['<0x94>', 151],\n",
       " ['<0x95>', 152],\n",
       " ['<0x96>', 153],\n",
       " ['<0x97>', 154],\n",
       " ['<0x98>', 155],\n",
       " ['<0x99>', 156],\n",
       " ['<0x9A>', 157],\n",
       " ['<0x9B>', 158],\n",
       " ['<0x9C>', 159],\n",
       " ['<0x9D>', 160],\n",
       " ['<0x9E>', 161],\n",
       " ['<0x9F>', 162],\n",
       " ['<0xA0>', 163],\n",
       " ['<0xA1>', 164],\n",
       " ['<0xA2>', 165],\n",
       " ['<0xA3>', 166],\n",
       " ['<0xA4>', 167],\n",
       " ['<0xA5>', 168],\n",
       " ['<0xA6>', 169],\n",
       " ['<0xA7>', 170],\n",
       " ['<0xA8>', 171],\n",
       " ['<0xA9>', 172],\n",
       " ['<0xAA>', 173],\n",
       " ['<0xAB>', 174],\n",
       " ['<0xAC>', 175],\n",
       " ['<0xAD>', 176],\n",
       " ['<0xAE>', 177],\n",
       " ['<0xAF>', 178],\n",
       " ['<0xB0>', 179],\n",
       " ['<0xB1>', 180],\n",
       " ['<0xB2>', 181],\n",
       " ['<0xB3>', 182],\n",
       " ['<0xB4>', 183],\n",
       " ['<0xB5>', 184],\n",
       " ['<0xB6>', 185],\n",
       " ['<0xB7>', 186],\n",
       " ['<0xB8>', 187],\n",
       " ['<0xB9>', 188],\n",
       " ['<0xBA>', 189],\n",
       " ['<0xBB>', 190],\n",
       " ['<0xBC>', 191],\n",
       " ['<0xBD>', 192],\n",
       " ['<0xBE>', 193],\n",
       " ['<0xBF>', 194],\n",
       " ['<0xC0>', 195],\n",
       " ['<0xC1>', 196],\n",
       " ['<0xC2>', 197],\n",
       " ['<0xC3>', 198],\n",
       " ['<0xC4>', 199],\n",
       " ['<0xC5>', 200],\n",
       " ['<0xC6>', 201],\n",
       " ['<0xC7>', 202],\n",
       " ['<0xC8>', 203],\n",
       " ['<0xC9>', 204],\n",
       " ['<0xCA>', 205],\n",
       " ['<0xCB>', 206],\n",
       " ['<0xCC>', 207],\n",
       " ['<0xCD>', 208],\n",
       " ['<0xCE>', 209],\n",
       " ['<0xCF>', 210],\n",
       " ['<0xD0>', 211],\n",
       " ['<0xD1>', 212],\n",
       " ['<0xD2>', 213],\n",
       " ['<0xD3>', 214],\n",
       " ['<0xD4>', 215],\n",
       " ['<0xD5>', 216],\n",
       " ['<0xD6>', 217],\n",
       " ['<0xD7>', 218],\n",
       " ['<0xD8>', 219],\n",
       " ['<0xD9>', 220],\n",
       " ['<0xDA>', 221],\n",
       " ['<0xDB>', 222],\n",
       " ['<0xDC>', 223],\n",
       " ['<0xDD>', 224],\n",
       " ['<0xDE>', 225],\n",
       " ['<0xDF>', 226],\n",
       " ['<0xE0>', 227],\n",
       " ['<0xE1>', 228],\n",
       " ['<0xE2>', 229],\n",
       " ['<0xE3>', 230],\n",
       " ['<0xE4>', 231],\n",
       " ['<0xE5>', 232],\n",
       " ['<0xE6>', 233],\n",
       " ['<0xE7>', 234],\n",
       " ['<0xE8>', 235],\n",
       " ['<0xE9>', 236],\n",
       " ['<0xEA>', 237],\n",
       " ['<0xEB>', 238],\n",
       " ['<0xEC>', 239],\n",
       " ['<0xED>', 240],\n",
       " ['<0xEE>', 241],\n",
       " ['<0xEF>', 242],\n",
       " ['<0xF0>', 243],\n",
       " ['<0xF1>', 244],\n",
       " ['<0xF2>', 245],\n",
       " ['<0xF3>', 246],\n",
       " ['<0xF4>', 247],\n",
       " ['<0xF5>', 248],\n",
       " ['<0xF6>', 249],\n",
       " ['<0xF7>', 250],\n",
       " ['<0xF8>', 251],\n",
       " ['<0xF9>', 252],\n",
       " ['<0xFA>', 253],\n",
       " ['<0xFB>', 254],\n",
       " ['<0xFC>', 255],\n",
       " ['<0xFD>', 256],\n",
       " ['<0xFE>', 257],\n",
       " ['<0xFF>', 258],\n",
       " ['▁t', 259],\n",
       " ['an', 260],\n",
       " ['in', 261],\n",
       " ['en', 262],\n",
       " ['er', 263],\n",
       " ['se', 264],\n",
       " ['or', 265],\n",
       " ['de', 266],\n",
       " ['is', 267],\n",
       " ['ra', 268],\n",
       " ['▁an', 269],\n",
       " ['ce', 270],\n",
       " ['ex', 271],\n",
       " ['he', 272],\n",
       " ['iz', 273],\n",
       " ['▁s', 274],\n",
       " ['ext', 275],\n",
       " ['sed', 276],\n",
       " ['▁in', 277],\n",
       " ['▁is', 278],\n",
       " ['▁the', 279],\n",
       " ['▁text', 280],\n",
       " ['Ne', 281],\n",
       " ['ag', 282],\n",
       " ['at', 283],\n",
       " ['gu', 284],\n",
       " ['ok', 285],\n",
       " ['pr', 286],\n",
       " ['te', 287],\n",
       " ['▁a', 288],\n",
       " ['▁b', 289],\n",
       " ['▁f', 290],\n",
       " ['▁l', 291],\n",
       " ['▁m', 292],\n",
       " ['▁u', 293],\n",
       " ['age', 294],\n",
       " ['det', 295],\n",
       " ['ent', 296],\n",
       " ['ese', 297],\n",
       " ['ing', 298],\n",
       " ['ral', 299],\n",
       " ['▁Ne', 300],\n",
       " ['▁pr', 301],\n",
       " ['angu', 302],\n",
       " ['ence', 303],\n",
       " ['eniz', 304],\n",
       " ['ural', 305],\n",
       " ['▁and', 306],\n",
       " ['▁for', 307],\n",
       " ['enizer', 308],\n",
       " ['▁langu', 309],\n",
       " ['okenizer', 310],\n",
       " ['▁language', 311],\n",
       " ['Ch', 312],\n",
       " ['En', 313],\n",
       " ['It', 314],\n",
       " ['Ja', 315],\n",
       " ['Pi', 316],\n",
       " ['ab', 317],\n",
       " ['ar', 318],\n",
       " ['ba', 319],\n",
       " ['cl', 320],\n",
       " ['ed', 321],\n",
       " ['et', 322],\n",
       " ['gl', 323],\n",
       " ['io', 324],\n",
       " ['it', 325],\n",
       " ['ly', 326],\n",
       " ['ms', 327],\n",
       " ['ne', 328],\n",
       " ['ns', 329],\n",
       " ['oc', 330],\n",
       " ['pu', 331],\n",
       " ['qu', 332],\n",
       " ['re', 333],\n",
       " ['tw', 334],\n",
       " ['ud', 335],\n",
       " ['ul', 336],\n",
       " ['up', 337],\n",
       " ['vi', 338],\n",
       " ['wh', 339],\n",
       " ['ys', 340],\n",
       " ['▁K', 341],\n",
       " ['▁S', 342],\n",
       " ['▁c', 343],\n",
       " ['▁g', 344],\n",
       " ['▁v', 345],\n",
       " ['Jap', 346],\n",
       " ['Pie', 347],\n",
       " ['ain', 348],\n",
       " ['ary', 349],\n",
       " ['ats', 350],\n",
       " ['del', 351],\n",
       " ['dep', 352],\n",
       " ['ean', 353],\n",
       " ['end', 354],\n",
       " ['ere', 355],\n",
       " ['erm', 356],\n",
       " ['etc', 357],\n",
       " ['ion', 358],\n",
       " ['ior', 359],\n",
       " ['ish', 360],\n",
       " ['ize', 361],\n",
       " ['ork', 362],\n",
       " ['▁', 363],\n",
       " ['e', 364],\n",
       " ['n', 365],\n",
       " ['t', 366],\n",
       " ['a', 367],\n",
       " ['i', 368],\n",
       " ['r', 369],\n",
       " ['s', 370],\n",
       " ['d', 371],\n",
       " ['o', 372],\n",
       " ['u', 373],\n",
       " ['l', 374],\n",
       " ['g', 375],\n",
       " ['c', 376],\n",
       " ['h', 377],\n",
       " ['p', 378],\n",
       " [',', 379],\n",
       " ['y', 380],\n",
       " ['b', 381],\n",
       " ['m', 382],\n",
       " ['k', 383],\n",
       " ['w', 384],\n",
       " ['x', 385],\n",
       " ['z', 386],\n",
       " ['.', 387],\n",
       " ['N', 388],\n",
       " ['f', 389],\n",
       " ['v', 390],\n",
       " ['-', 391],\n",
       " ['C', 392],\n",
       " ['E', 393],\n",
       " ['I', 394],\n",
       " ['J', 395],\n",
       " ['K', 396],\n",
       " ['P', 397],\n",
       " ['S', 398],\n",
       " ['q', 399]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"tok400.model\")\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29ab32e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[342, 296, 303, 347, 270, 278, 269, 293, 329, 337, 263, 338, 276, 280, 259, 310, 306, 363, 295, 310, 387]\n"
     ]
    }
   ],
   "source": [
    "ids = sp.encode(\"SentencePiece is an unsupervised text tokenizer and detokenizer.\")\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72d3fe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁S', 'ent', 'ence', 'Pie', 'ce', '▁is', '▁an', '▁u', 'ns', 'up', 'er', 'vi', 'sed', '▁text', '▁t', 'okenizer', '▁and', '▁', 'det', 'okenizer', '.']\n"
     ]
    }
   ],
   "source": [
    "print([sp.id_to_piece(idx) for idx in ids])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
